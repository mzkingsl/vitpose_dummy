{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source\n",
    "-- https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/solutions/04_pytorch_custom_datasets_exercise_solutions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skimage'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcnnDataset\u001b[39;00m \u001b[39mimport\u001b[39;00m CNNDataset\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m ImageFolder\n",
      "File \u001b[1;32mc:\\Users\\mkingsl6\\Desktop\\ViTPose\\298_scripts\\CNN_test\\cnnDataset.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mskimage\u001b[39;00m \u001b[39mimport\u001b[39;00m io \n\u001b[0;32m      8\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCNNDataset\u001b[39;00m(Dataset):\n\u001b[0;32m      9\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m    Dataset class for the CNN clasification model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m        transform (callable, optional): Optional transform to be applied\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'skimage'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from cnnDataset import CNNDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "resizer = 1\n",
    "transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Resize([64*resizer, 64*resizer]), # This line\n",
    "                               torchvision.transforms.Normalize((.5,.5,.5), (.5,.5,.5))\n",
    "                                ])\n",
    "# load dataset \n",
    "# csv_path = 'C:\\\\Users\\\\amart50\\\\Desktop\\\\CNN_test\\\\cnn_name_class.csv'\n",
    "# data_path = 'C:\\\\Users\\\\amart50\\\\Desktop\\\\CNN_test\\\\Data_Cnn'\n",
    "\n",
    "# dataset = CNNDataset(csv_file='cnn_name_class.csv', root_dir='Data_cnn', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'C:\\\\Users\\\\amart50\\\\Desktop\\\\CNN_test\\\\data'\n",
    "dataset = ImageFolder(data_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "838 210 1048 1048\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "train_split = int( len(dataset) * .8)\n",
    "test_split = len(dataset)- train_split\n",
    "print(train_split, test_split, train_split+test_split, len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [train_split, test_split])\n",
    "trainloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(test_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Bench', 'Deadlift', 'Squat'],\n",
       " <torch.utils.data.dataloader.DataLoader at 0x27f8b610ca0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x27f8331b4c0>)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset.__getitem__(0)\n",
    "dataset.classes, trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape:torch.Size([1, 3, 64, 64]) (B, corlorchannel, H, W)\n",
      "Image Shape:torch.Size([1]) (B, corlorchannel, H, W)\n"
     ]
    }
   ],
   "source": [
    "img, label = next(iter(trainloader))\n",
    "print(f\"Image Shape:{img.shape} (B, corlorchannel, H, W)\")\n",
    "print(f\"Image Shape:{label.shape} (B, corlorchannel, H, W)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x864 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# grid_img = torchvision.utils.make_grid(images[:4])\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(grid_img.permute(1, 2, 0))\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_batch(dl):\n",
    "    \"\"\"Plot images grid of single batch\"\"\"\n",
    "    for images, labels in dl:\n",
    "        fig,ax = plt.subplots(figsize = (16,12))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images,nrow=16).permute(1,2,0))\n",
    "        break\n",
    "        \n",
    "show_batch(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TinyVGG(nn.Module):\n",
    "  def __init__(self, input_shape, hidden_units, output_shape):\n",
    "    super().__init__()\n",
    "    self.conv_block_1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=input_shape,\n",
    "                  out_channels=2* hidden_units,\n",
    "                  kernel_size=3,\n",
    "                  stride=1,\n",
    "                  padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=2* hidden_units,\n",
    "                  out_channels=4* hidden_units,\n",
    "                  kernel_size=3,\n",
    "                  stride=1,\n",
    "                  padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2)\n",
    "    )\n",
    "    self.conv_block_2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=4* hidden_units,\n",
    "                  out_channels=hidden_units,\n",
    "                  kernel_size=3,\n",
    "                  stride=1,\n",
    "                  padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=hidden_units,\n",
    "                  out_channels=hidden_units,\n",
    "                  kernel_size=3,\n",
    "                  stride=1,\n",
    "                  padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2)\n",
    "    )\n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=hidden_units*16*16,\n",
    "                  out_features=output_shape))\n",
    "        \n",
    "  def forward(self, x):\n",
    "    x = self.conv_block_1(x)\n",
    "    # print(f\"Layer 1 shape: {x.shape}\")\n",
    "    x = self.conv_block_2(x)\n",
    "    # print(f\"Layer 2 shape: {x.shape}\")\n",
    "    x = self.classifier(x)\n",
    "    # print(f\"Layer 3 shape: {x.shape}\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyVGG(\n",
       "  (conv_block_1): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_block_2): Sequential(\n",
       "    (0): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=4096, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0 = TinyVGG(input_shape = 3,\n",
    "                  hidden_units=16,\n",
    "                  output_shape=len(dataset.classes)).to(device)\n",
    "model_0\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0061, -0.0148, -0.0074]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass dummy data through model\n",
    "dummy_x = torch.rand(size=[1, 3, 64, 64])\n",
    "model_0(dummy_x.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer):\n",
    "  \n",
    "  # Put the model in train mode\n",
    "  model.train()\n",
    "\n",
    "  # Setup train loss and train accuracy values\n",
    "  train_loss, train_acc = 0, 0\n",
    "\n",
    "  # Loop through data loader and data batches\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "    # Send data to target device\n",
    "    X, y = X.to(device), y.to(device) \n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_pred = model(X)\n",
    "    # print(y_pred)\n",
    "\n",
    "    # 2. Calculate and accumulate loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    train_loss += loss.item()\n",
    "\n",
    "    # 3. Optimizer zero grad \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backward \n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculate and accumualte accuracy metric across all batches\n",
    "    y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "    train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "  # Adjust metrics to get average loss and average accuracy per batch\n",
    "  train_loss = train_loss / len(dataloader)\n",
    "  train_acc = train_acc / len(dataloader)\n",
    "  return train_loss, train_acc \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module):\n",
    "  \n",
    "  # Put model in eval mode\n",
    "  model.eval()\n",
    "\n",
    "  # Setup the test loss and test accuracy values\n",
    "  test_loss, test_acc = 0, 0\n",
    "\n",
    "  # Turn on inference context manager\n",
    "  with torch.inference_mode():\n",
    "    # Loop through DataLoader batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "      # Send data to target device\n",
    "      X, y = X.to(device), y.to(device)\n",
    "\n",
    "      # 1. Forward pass\n",
    "      test_pred_logits = model(X)\n",
    "      # print(test_pred_logits)\n",
    "\n",
    "      # 2. Calculuate and accumulate loss\n",
    "      loss = loss_fn(test_pred_logits, y)\n",
    "      test_loss += loss.item()\n",
    "\n",
    "      # Calculate and accumulate accuracy\n",
    "      test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "      test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "    \n",
    "  # Adjust metrics to get average loss and accuracy per batch\n",
    "  test_loss = test_loss / len(dataloader)\n",
    "  test_acc = test_acc / len(dataloader)\n",
    "  return test_loss, test_acc\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs: int = 5):\n",
    "  \n",
    "  # Create results dictionary\n",
    "  results = {\"train_loss\": [],\n",
    "             \"train_acc\": [],\n",
    "             \"test_loss\": [],\n",
    "             \"test_acc\": []}\n",
    "\n",
    "  # Loop through the training and testing steps for a number of epochs\n",
    "  for epoch in tqdm(range(epochs)):\n",
    "    # Train step\n",
    "    train_loss, train_acc = train_step(model=model, \n",
    "                                       dataloader=train_dataloader,\n",
    "                                       loss_fn=loss_fn,\n",
    "                                       optimizer=optimizer)\n",
    "    # Test step\n",
    "    test_loss, test_acc = test_step(model=model, \n",
    "                                    dataloader=test_dataloader,\n",
    "                                    loss_fn=loss_fn)\n",
    "    \n",
    "    # Print out what's happening\n",
    "    print(f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Update the results dictionary\n",
    "    results[\"train_loss\"].append(train_loss)\n",
    "    results[\"train_acc\"].append(train_acc)\n",
    "    results[\"test_loss\"].append(test_loss)\n",
    "    results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "  # Return the results dictionary\n",
    "  return results\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = TinyVGG(input_shape=3,\n",
    "                  hidden_units=10,\n",
    "                  output_shape=len(dataset.classes)).to(device)\n",
    "\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_0.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5338646c4751467c94f7b845c862e69a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0901 | train_acc: 0.4141 | test_loss: 1.0806 | test_acc: 0.4333\n",
      "Epoch: 2 | train_loss: 1.0886 | train_acc: 0.4177 | test_loss: 1.0789 | test_acc: 0.4333\n",
      "Epoch: 3 | train_loss: 1.0864 | train_acc: 0.4177 | test_loss: 1.0784 | test_acc: 0.4333\n",
      "Epoch: 4 | train_loss: 1.0884 | train_acc: 0.4177 | test_loss: 1.0802 | test_acc: 0.4333\n",
      "Epoch: 5 | train_loss: 1.0867 | train_acc: 0.4177 | test_loss: 1.0821 | test_acc: 0.4333\n"
     ]
    }
   ],
   "source": [
    "model_0_results = train(model=model_0,\n",
    "                        train_dataloader=trainloader,\n",
    "                        test_dataloader=testloader,\n",
    "                        optimizer=optimizer,\n",
    "                        epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(num_epochs):\n",
    "#     losses = []\n",
    "#     for batch_idx, (data, targets) in enumerate(trainloader):\n",
    "#         # Get data to cuda if possible\n",
    "#         data = data.to(device=device)\n",
    "#         targets = targets.to(device=device)\n",
    "        \n",
    "#         # forward\n",
    "#         scores = model(data)\n",
    "#         loss = criterion(scores, targets)\n",
    "#         losses.append(loss.item())\n",
    "        \n",
    "#         # backward\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "        \n",
    "#         # gradient descent or adam step\n",
    "#         optimizer.step()\n",
    "        \n",
    "#     print(f'Cost at epoch {epoch} is {sum(losses)/len(losses):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     \"\"\"\n",
    "#     CNN model for lift classification with 3 output layers classifying \n",
    "#     the lift as Squat, Bench, or Deadlift\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         #define a 3 layer CNN\n",
    "#         self.conv1 = nn.Conv2d(3,1 , 256) # 1 input channel, 32 output channels, 3x3 kernel, stride 1, padding 1\n",
    "#         self.conv2 = nn.Conv2d(256, 256*2, 5) # 32 input channels, 64 output channels, 3x3 kernel, stride 1, padding 1\n",
    "#         self.maxpool_layer = nn.MaxPool2d(3, 3) # 2x2 kernel, stride 2\n",
    "#         self.fc_layer = nn.Linear(256*5*5, 3) # 64*5*5 input features, 3 output features (for 3 classes)\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.maxpool_layer(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.maxpool_layer(x)\n",
    "#         x = x.view(-1, 256*5*5) \n",
    "#         x = torch.sigmoid(self.fc_layer(x))\n",
    "#         return x\n",
    "    \n",
    "\n",
    "#     # def test_one(self, image, label):\n",
    "#     #     outputs = self(image)\n",
    "#     #     _, predicted = torch.max(outputs.data, 1)\n",
    "#     #     print('Predicted: ', predicted.item(), 'Actual: ', label.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'Classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\amart50\\Desktop\\CNN_test\\cnn.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# import cv2\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# x = iter(trainloader)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# images, labels = next(x)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m trainloader\u001b[39m.\u001b[39;49mClasses\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'Classes'"
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# x = iter(trainloader)\n",
    "# images, labels = next(x)\n",
    "trainloader.Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x864 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# grid_img = torchvision.utils.make_grid(images[:4])\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(grid_img.permute(1, 2, 0))\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_batch(dl):\n",
    "    \"\"\"Plot images grid of single batch\"\"\"\n",
    "    for images, labels in dl:\n",
    "        fig,ax = plt.subplots(figsize = (16,12))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images,nrow=16).permute(1,2,0))\n",
    "        break\n",
    "        \n",
    "show_batch(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the dimensions of the input image\n",
    "width = 256\n",
    "height = 256\n",
    "\n",
    "# Define the number of classes in the dataset\n",
    "num_classes = 3\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.conv1 = torch.nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv2 = torch.nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv3 = torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "    self.fc1 = torch.nn.Linear(64 * width * height, 512)\n",
    "    self.fc2 = torch.nn.Linear(512, num_classes)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = torch.nn.functional.relu(self.conv1(x))\n",
    "    x = torch.nn.functional.relu(self.conv2(x))\n",
    "    x = torch.nn.functional.relu(self.conv3(x))\n",
    "    x = x.view(-1, 64 * width * height)\n",
    "    x = torch.nn.functional.relu(self.fc1(x))\n",
    "    x = self.fc2(x)\n",
    "    return x\n",
    "\n",
    "# Create an instance of the neural network\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer):\n",
    "  # Set the model to training mode\n",
    "  model.train()\n",
    "\n",
    "  # Initialize the average loss\n",
    "  avg_loss = 0.0\n",
    "\n",
    "  # Loop over the training data\n",
    "  for images, labels in data:\n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimize the model's parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update the average loss\n",
    "    avg_loss += loss.item()\n",
    "\n",
    "  # Return the average loss\n",
    "  return avg_loss / len(data)\n",
    "\n",
    "\n",
    "def evaluate(model, data):\n",
    "  # Set the model to evaluation mode\n",
    "  model.eval()\n",
    "\n",
    "  # Initialize the average accuracy\n",
    "  avg_accuracy = 0.0\n",
    "\n",
    "  # Loop over the validation data\n",
    "  for images, labels in data:\n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "\n",
    "    # Compute the accuracy\n",
    "    _, predictions = torch.max(outputs, dim=1)\n",
    "    accuracy = torch.mean(predictions.eq(labels).float())\n",
    "\n",
    "    # Update the average accuracy\n",
    "    avg_accuracy += accuracy.item()\n",
    "\n",
    "  # Return the average accuracy\n",
    "  return avg_accuracy / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\amart50\\Desktop\\CNN_test\\cnn.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X54sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m valid_accuracies \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X54sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X54sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m   \u001b[39m# Train the model for one epoch\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X54sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m   train_accuracy \u001b[39m=\u001b[39m train(model, trainloader, optimizer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X54sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m   train_accuracies\u001b[39m.\u001b[39mappend(train_accuracy)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X54sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m   \u001b[39m# Evaluate the model on the validation set\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\amart50\\Desktop\\CNN_test\\cnn.ipynb Cell 19\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, data, optimizer)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X54sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X54sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Optimize the model's parameters\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X54sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X54sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Update the average loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amart50/Desktop/CNN_test/cnn.ipynb#X54sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m avg_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    235\u001b[0m          grads,\n\u001b[0;32m    236\u001b[0m          exp_avgs,\n\u001b[0;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    239\u001b[0m          state_steps,\n\u001b[0;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[0;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 300\u001b[0m func(params,\n\u001b[0;32m    301\u001b[0m      grads,\n\u001b[0;32m    302\u001b[0m      exp_avgs,\n\u001b[0;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    305\u001b[0m      state_steps,\n\u001b[0;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\adam.py:410\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    408\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    409\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 410\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    412\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train the model and record its accuracy on the training set and validation set\n",
    "# at each epoch\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "for epoch in range(num_epochs):\n",
    "  # Train the model for one epoch\n",
    "  train_accuracy = train(model, trainloader, optimizer)\n",
    "  train_accuracies.append(train_accuracy)\n",
    "  \n",
    "  # Evaluate the model on the validation set\n",
    "  valid_accuracy = evaluate(model, testloader)\n",
    "  valid_accuracies.append(valid_accuracy)\n",
    "\n",
    "# Create a plot of the model's accuracy versus the number of epochs\n",
    "plt.plot(range(1, num_epochs + 1), train_accuracies, label=\"Train\")\n",
    "plt.plot(range(1, num_epochs + 1), valid_accuracies, label=\"Valid\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define the initial learning rate here\n",
    "# learning_rate = 1e-2\n",
    "# n_epochs = 30 # how many epochs to run\n",
    "\n",
    "# # define loss function\n",
    "# criterion = nn.BCELoss()\n",
    "# cnn_net = Net()\n",
    "# optimizer = torch.optim.SGD(cnn_net.parameters(), lr=learning_rate)\n",
    "\n",
    "# for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(trainloader, 1):\n",
    "\n",
    "#         # get the inputs; data is a list of [inputs, labels]\n",
    "#         inputs, labels = data\n",
    "#         labels = labels.float()\n",
    "\n",
    "#         # Forward \n",
    "#         output = cnn_net(inputs)\n",
    "        \n",
    "#         # Compute the loss using the final output\n",
    "#         loss = criterion(output, labels)\n",
    "\n",
    "#         # Backpropagation\n",
    "#         # YOUR CODE HERE\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         #raise NotImplementedError()\n",
    "        \n",
    "#         # print statistics\n",
    "#         running_loss += loss.item()\n",
    "#         if i % 200 == 199:  # print every 200 mini-batches\n",
    "#             print('[Epoch %d, Step %5d] loss: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / 200))\n",
    "#             running_loss = 0.0\n",
    "\n",
    "# print('Finished Training')aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CnNDataset debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "annotations = pd.read_csv('cnn_name_class.csv')\n",
    "root_dir = 'C:\\\\Users\\\\amart50\\\\Desktop\\\\CNN_test\\\\Data_Cnn'\n",
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\amart50\\\\Desktop\\\\CNN_test\\\\Data_Cnn', 'bench1.jpg')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir, annotations.iloc[index, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\amart50\\\\Desktop\\\\CNN_test\\\\Data_Cnn\\\\bench1.jpg'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = os.path.join(root_dir, annotations.iloc[index, 1])\n",
    "img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = annotations['Names']\n",
    "new_name = []\n",
    "for names in name:\n",
    "    names = names.split(\"/\")[1]\n",
    "    new_name.append(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Names</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bench/bench0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bench/bench1.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bench/bench10.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bench/bench100.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bench/bench101.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Names  Class\n",
       "0    bench/bench0.jpg      0\n",
       "1    bench/bench1.jpg      0\n",
       "2   bench/bench10.jpg      0\n",
       "3  bench/bench100.jpg      0\n",
       "4  bench/bench101.jpg      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations['Names'].replace(new_name)\n",
    "annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1043, 1043)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations), len(new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Names</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bench/bench0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bench/bench1.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bench/bench10.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bench/bench100.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bench/bench101.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Names  Class\n",
       "0    bench/bench0.jpg      0\n",
       "1    bench/bench1.jpg      0\n",
       "2   bench/bench10.jpg      0\n",
       "3  bench/bench100.jpg      0\n",
       "4  bench/bench101.jpg      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations[\"Names\"] = new_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.to_csv(\"cnn_name_class.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9167cefd11376d22eea86c15b13ce1bc8473a1d8eae457488847feecbe89a0eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
