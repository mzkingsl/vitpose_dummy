{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import os \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_v2 = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import images\n",
    "#change user\n",
    "path = \"C:\\\\Users\\\\mkingsl6\\\\Desktop\\\\ViTPose-main\\\\LiftingData_cnn\"\n",
    "deadlift = path + \"\\\\Deadlift\"\n",
    "bench = path + \"\\\\Bench\"\n",
    "squats = path + \"\\\\Squat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert deadlift to jpg\n",
    "pos_paths_list = [deadlift, bench, squats]\n",
    "\n",
    "filenames= []\n",
    "for pos_paths in pos_paths_list:\n",
    "    files = []\n",
    "    for file in os.listdir(pos_paths):\n",
    "        # check the files which are end with specific extension\n",
    "        if file.endswith(\".png\"):\n",
    "            # print path name of selected files\n",
    "            file_path = os.path.join(pos_paths, file)\n",
    "            # print(file_path)\n",
    "            im = Image.open(file_path)\n",
    "            new_name = file.split(\".\")[0] + \".jpg\"\n",
    "            # filenames.append(new_name)\n",
    "            # files.append(new_name)\n",
    "            # im.save(new_name)\n",
    "            Image.open(file_path).convert('RGB').save(pos_paths+\"\\\\\"+new_name)\n",
    "\n",
    "\n",
    "    for file in os.listdir(pos_paths):\n",
    "        # check the files which are end with specific extension\n",
    "        if file.endswith(\".jpeg\"):\n",
    "            # print path name of selected files\n",
    "            file_path = os.path.join(pos_paths, file)\n",
    "            # print(file_path)\n",
    "            im = Image.open(file_path)\n",
    "            new_name = file.split(\".\")[0] + \".jpg\"\n",
    "            \n",
    "            \n",
    "            # filenames.append(new_name)\n",
    "            # im.save(new_name)\n",
    "            Image.open(file_path).convert('RGB').save(pos_paths+\"\\\\\"+new_name)\n",
    "    for file in os.listdir(pos_paths):\n",
    "        files.append(new_name)\n",
    "    filenames.append(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # normalise the pixel values\n",
    "# train_ = ImageDataGenerator(validation_split = .2, rescale=1./255)\n",
    "# TRAIN_DIR = path\n",
    "# train_dataset = train_.flow_from_directory(\n",
    "#     TRAIN_DIR,\n",
    "#     target_size = (256, 256),\n",
    "#     subset='training',\n",
    "#     batch_size = 10,\n",
    "#     class_mode='categorical'\n",
    "# )\n",
    "\n",
    "# validation_dataset = train_.flow_from_directory(\n",
    "#     TRAIN_DIR,\n",
    "#     target_size = (256, 256),\n",
    "#     subset='validation',\n",
    "#     class_mode='categorical',\n",
    "#     batch_size = 10,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function(filename, label):\n",
    "    \"\"\"Function that returns a tuple of normalized image array and labels array.\n",
    "    Args:\n",
    "        filename: string representing path to image\n",
    "        label: 0/1 one-dimensional array of size N_LABELS\n",
    "    \"\"\"\n",
    "    # Read an image from a file\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    # Decode it into a dense vector\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    # Resize it to fixed shape\n",
    "    image_resized = tf.image.resize(image_decoded, [256, 256])\n",
    "    # Normalize it from [0, 255] to [0.0, 1.0]\n",
    "    image_normalized = image_resized / 255.0\n",
    "    return image_normalized, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't convert non-rectangular Python sequence to Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\mkingsl6\\.conda\\envs\\cnn_notebook\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:104\u001b[0m, in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    103\u001b[0m   \u001b[39mif\u001b[39;00m spec \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m     spec \u001b[39m=\u001b[39m type_spec_from_value(t, use_fallback\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    105\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    106\u001b[0m   \u001b[39m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[0;32m    107\u001b[0m   \u001b[39m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mkingsl6\\.conda\\envs\\cnn_notebook\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:491\u001b[0m, in \u001b[0;36mtype_spec_from_value\u001b[1;34m(element, use_fallback)\u001b[0m\n\u001b[0;32m    488\u001b[0m     logging\u001b[39m.\u001b[39mvlog(\n\u001b[0;32m    489\u001b[0m         \u001b[39m3\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to convert \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m to tensor: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mtype\u001b[39m(element)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, e))\n\u001b[1;32m--> 491\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCould not build a `TypeSpec` for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m with type \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    492\u001b[0m     element,\n\u001b[0;32m    493\u001b[0m     \u001b[39mtype\u001b[39m(element)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: Could not build a `TypeSpec` for [['99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg'], ['99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg'], ['99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg', '99.jpg']] with type list",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [49], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m AUTOTUNE \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mAUTOTUNE \u001b[39m# Adapt preprocessing and prefetching dynamically to reduce GPU and CPU idle time\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# Create a first dataset of file paths and labels\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mDataset\u001b[39m.\u001b[39;49mfrom_tensor_slices((filenames, [\u001b[39m'\u001b[39;49m\u001b[39msquat\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mbench\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mdeadlift\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n\u001b[0;32m      6\u001b[0m \u001b[39m# Parse and preprocess observations in parallel\u001b[39;00m\n\u001b[0;32m      7\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mmap(parse_function, num_parallel_calls\u001b[39m=\u001b[39mAUTOTUNE)\n",
      "File \u001b[1;32mc:\\Users\\mkingsl6\\.conda\\envs\\cnn_notebook\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:818\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[39m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[0;32m    816\u001b[0m \u001b[39m# from_tensor_slices_op -> dataset_ops).\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m from_tensor_slices_op  \u001b[39m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[1;32m--> 818\u001b[0m \u001b[39mreturn\u001b[39;00m from_tensor_slices_op\u001b[39m.\u001b[39;49mfrom_tensor_slices(tensors, name)\n",
      "File \u001b[1;32mc:\\Users\\mkingsl6\\.conda\\envs\\cnn_notebook\\lib\\site-packages\\tensorflow\\python\\data\\ops\\from_tensor_slices_op.py:25\u001b[0m, in \u001b[0;36mfrom_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_tensor_slices\u001b[39m(tensors, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> 25\u001b[0m   \u001b[39mreturn\u001b[39;00m TensorSliceDataset(tensors, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\mkingsl6\\.conda\\envs\\cnn_notebook\\lib\\site-packages\\tensorflow\\python\\data\\ops\\from_tensor_slices_op.py:33\u001b[0m, in \u001b[0;36mTensorSliceDataset.__init__\u001b[1;34m(self, element, is_files, name)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, element, is_files\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     32\u001b[0m   \u001b[39m\"\"\"See `Dataset.from_tensor_slices` for details.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m   element \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39;49mnormalize_element(element)\n\u001b[0;32m     34\u001b[0m   batched_spec \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mtype_spec_from_value(element)\n\u001b[0;32m     35\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001b[1;32mc:\\Users\\mkingsl6\\.conda\\envs\\cnn_notebook\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:109\u001b[0m, in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    104\u001b[0m     spec \u001b[39m=\u001b[39m type_spec_from_value(t, use_fallback\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    105\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    106\u001b[0m   \u001b[39m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[0;32m    107\u001b[0m   \u001b[39m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m   normalized_components\u001b[39m.\u001b[39mappend(\n\u001b[1;32m--> 109\u001b[0m       ops\u001b[39m.\u001b[39;49mconvert_to_tensor(t, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcomponent_\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m%\u001b[39;49m i))\n\u001b[0;32m    110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(spec, sparse_tensor\u001b[39m.\u001b[39mSparseTensorSpec):\n",
      "File \u001b[1;32mc:\\Users\\mkingsl6\\.conda\\envs\\cnn_notebook\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mkingsl6\\.conda\\envs\\cnn_notebook\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1636\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1627\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1628\u001b[0m           _add_error_prefix(\n\u001b[0;32m   1629\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1632\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1633\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[0;32m   1635\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1636\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[0;32m   1638\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[0;32m   1639\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mkingsl6\\.conda\\envs\\cnn_notebook\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:343\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    341\u001b[0m                                          as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    342\u001b[0m   _ \u001b[39m=\u001b[39m as_ref\n\u001b[1;32m--> 343\u001b[0m   \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\mkingsl6\\.conda\\envs\\cnn_notebook\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[0;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    172\u001b[0m   \u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \n\u001b[0;32m    174\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    268\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\mkingsl6\\.conda\\envs\\cnn_notebook\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    278\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 279\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m    281\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[0;32m    282\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
      "File \u001b[1;32mc:\\Users\\mkingsl6\\.conda\\envs\\cnn_notebook\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[0;32m    303\u001b[0m   \u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[0;32m    305\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[1;32mc:\\Users\\mkingsl6\\.conda\\envs\\cnn_notebook\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Can't convert non-rectangular Python sequence to Tensor."
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE # Adapt preprocessing and prefetching dynamically to reduce GPU and CPU idle time\n",
    "\n",
    "\n",
    "# Create a first dataset of file paths and labels\n",
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, ['squat','bench','deadlift']))\n",
    "# Parse and preprocess observations in parallel\n",
    "dataset = dataset.map(parse_function, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "\n",
    "# This is a small dataset, only load it once, and keep it in memory.\n",
    "dataset = dataset.cache()\n",
    "    # Shuffle the data each buffer size\n",
    "dataset = dataset.shuffle(buffer_size=1024)\n",
    "        \n",
    "# Batch the data for multiple steps\n",
    "dataset = dataset.batch(256)\n",
    "# Fetch batches in the background while the model is training.\n",
    "dataset = dataset.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_url = \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\"\n",
    "feature_extractor_layer = hub.KerasLayer(feature_extractor_url,\n",
    "                                         input_shape=(256,256,3))\n",
    "feature_extractor_layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"keras_layer_2\" (type KerasLayer).\n\nin user code:\n\n    File \"c:\\Users\\mkingsl6\\.conda\\envs\\cnn_notebook\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\", line 237, in call  *\n        result = smart_cond.smart_cond(training,\n\n    ValueError: Could not find matching concrete function to call loaded from the SavedModel. Got:\n      Positional arguments (4 total):\n        * <tf.Tensor 'inputs:0' shape=(None, 256, 256, 3) dtype=float32>\n        * False\n        * False\n        * 0.99\n      Keyword arguments: {}\n    \n     Expected these arguments to match one of the following 4 option(s):\n    \n    Option 1:\n      Positional arguments (4 total):\n        * TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n        * False\n        * False\n        * TensorSpec(shape=(), dtype=tf.float32, name='batch_norm_momentum')\n      Keyword arguments: {}\n    \n    Option 2:\n      Positional arguments (4 total):\n        * TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n        * False\n        * True\n        * TensorSpec(shape=(), dtype=tf.float32, name='batch_norm_momentum')\n      Keyword arguments: {}\n    \n    Option 3:\n      Positional arguments (4 total):\n        * TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n        * True\n        * True\n        * TensorSpec(shape=(), dtype=tf.float32, name='batch_norm_momentum')\n      Keyword arguments: {}\n    \n    Option 4:\n      Positional arguments (4 total):\n        * TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n        * True\n        * False\n        * TensorSpec(shape=(), dtype=tf.float32, name='batch_norm_momentum')\n      Keyword arguments: {}\n\n\nCall arguments received by layer \"keras_layer_2\" (type KerasLayer):\n  • inputs=tf.Tensor(shape=(None, 256, 256, 3), dtype=float32)\n  • training=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m neural_net \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mSequential([\n\u001b[0;32m      2\u001b[0m   feature_extractor_layer,\n\u001b[0;32m      3\u001b[0m   \u001b[39m# tf.keras.layers.Dropout(0.3),\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m   tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mDense(\u001b[39m1024\u001b[39;49m, activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m'\u001b[39;49m, name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhidden_layer\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m      5\u001b[0m   tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mDense(\u001b[39m3\u001b[39;49m, activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msigmoid\u001b[39;49m\u001b[39m'\u001b[39;49m, name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39moutput\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m ])\n",
      "File \u001b[1;32mc:\\Users\\mkingsl6\\.conda\\envs\\cnn_notebook\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mkingsl6\\.conda\\envs\\cnn_notebook\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Scratch\\__autograph_generated_filenz_l719t.py:74\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m     72\u001b[0m     result \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(smart_cond)\u001b[39m.\u001b[39msmart_cond, (ag__\u001b[39m.\u001b[39mld(training), ag__\u001b[39m.\u001b[39mautograph_artifact(\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(f), (), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), fscope)), ag__\u001b[39m.\u001b[39mautograph_artifact(\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(f), (), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m), fscope))), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     73\u001b[0m result \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 74\u001b[0m ag__\u001b[39m.\u001b[39;49mif_stmt(ag__\u001b[39m.\u001b[39;49mnot_(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_has_training_argument), if_body_3, else_body_3, get_state_3, set_state_3, (\u001b[39m'\u001b[39;49m\u001b[39mresult\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtraining\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_6\u001b[39m():\n\u001b[0;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m (result,)\n",
      "File \u001b[1;32mC:\\Scratch\\__autograph_generated_filenz_l719t.py:72\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.else_body_3\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m     training \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     71\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mtrainable, if_body_2, else_body_2, get_state_2, set_state_2, (\u001b[39m'\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m result \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(smart_cond)\u001b[39m.\u001b[39;49msmart_cond, (ag__\u001b[39m.\u001b[39;49mld(training), ag__\u001b[39m.\u001b[39;49mautograph_artifact(\u001b[39mlambda\u001b[39;49;00m : ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(f), (), \u001b[39mdict\u001b[39;49m(training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), fscope)), ag__\u001b[39m.\u001b[39;49mautograph_artifact(\u001b[39mlambda\u001b[39;49;00m : ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(f), (), \u001b[39mdict\u001b[39;49m(training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m), fscope))), \u001b[39mNone\u001b[39;49;00m, fscope)\n",
      "File \u001b[1;32mC:\\Scratch\\__autograph_generated_filenz_l719t.py:72\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.else_body_3.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m     training \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     71\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mtrainable, if_body_2, else_body_2, get_state_2, set_state_2, (\u001b[39m'\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m result \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(smart_cond)\u001b[39m.\u001b[39msmart_cond, (ag__\u001b[39m.\u001b[39mld(training), ag__\u001b[39m.\u001b[39mautograph_artifact(\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(f), (), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), fscope)), ag__\u001b[39m.\u001b[39mautograph_artifact(\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(f), (), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m), fscope))), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"keras_layer_2\" (type KerasLayer).\n\nin user code:\n\n    File \"c:\\Users\\mkingsl6\\.conda\\envs\\cnn_notebook\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\", line 237, in call  *\n        result = smart_cond.smart_cond(training,\n\n    ValueError: Could not find matching concrete function to call loaded from the SavedModel. Got:\n      Positional arguments (4 total):\n        * <tf.Tensor 'inputs:0' shape=(None, 256, 256, 3) dtype=float32>\n        * False\n        * False\n        * 0.99\n      Keyword arguments: {}\n    \n     Expected these arguments to match one of the following 4 option(s):\n    \n    Option 1:\n      Positional arguments (4 total):\n        * TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n        * False\n        * False\n        * TensorSpec(shape=(), dtype=tf.float32, name='batch_norm_momentum')\n      Keyword arguments: {}\n    \n    Option 2:\n      Positional arguments (4 total):\n        * TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n        * False\n        * True\n        * TensorSpec(shape=(), dtype=tf.float32, name='batch_norm_momentum')\n      Keyword arguments: {}\n    \n    Option 3:\n      Positional arguments (4 total):\n        * TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n        * True\n        * True\n        * TensorSpec(shape=(), dtype=tf.float32, name='batch_norm_momentum')\n      Keyword arguments: {}\n    \n    Option 4:\n      Positional arguments (4 total):\n        * TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n        * True\n        * False\n        * TensorSpec(shape=(), dtype=tf.float32, name='batch_norm_momentum')\n      Keyword arguments: {}\n\n\nCall arguments received by layer \"keras_layer_2\" (type KerasLayer):\n  • inputs=tf.Tensor(shape=(None, 256, 256, 3), dtype=float32)\n  • training=None"
     ]
    }
   ],
   "source": [
    "neural_net = tf.keras.Sequential([\n",
    "  feature_extractor_layer,\n",
    "  # tf.keras.layers.Dropout(0.3),\n",
    "  tf.keras.layers.Dense(1024, activation='relu', name='hidden_layer'),\n",
    "  tf.keras.layers.Dense(3, activation='sigmoid', name='output')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 1280)              2257984   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 3843      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,261,827\n",
      "Trainable params: 3,843\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "neural_net.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid keyword argument(s) in `compile()`: ({'model_metrics', 'model_loss', 'model_optimizer'},). Valid keyword arguments include \"cloning\", \"experimental_run_tf_function\", \"distribute\", \"target_tensors\", or \"sample_weight_mode\".",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m neural_net\u001b[39m.\u001b[39;49mcompile(\n\u001b[0;32m      2\u001b[0m   model_optimizer\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49moptimizers\u001b[39m.\u001b[39;49mAdam(),\n\u001b[0;32m      3\u001b[0m   model_loss\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlosses\u001b[39m.\u001b[39;49mCategoricalCrossentropy(from_logits\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[0;32m      4\u001b[0m   model_metrics\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39macc\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\mkingsl6\\.conda\\envs\\cnn_notebook\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\mkingsl6\\.conda\\envs\\cnn_notebook\\lib\\site-packages\\keras\\engine\\training.py:3602\u001b[0m, in \u001b[0;36mModel._validate_compile\u001b[1;34m(self, optimizer, metrics, **kwargs)\u001b[0m\n\u001b[0;32m   3600\u001b[0m invalid_kwargs \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(kwargs) \u001b[39m-\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39msample_weight_mode\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m   3601\u001b[0m \u001b[39mif\u001b[39;00m invalid_kwargs:\n\u001b[1;32m-> 3602\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   3603\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInvalid keyword argument(s) in `compile()`: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3604\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m(invalid_kwargs,)\u001b[39m}\u001b[39;00m\u001b[39m. Valid keyword arguments include \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3605\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcloning\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexperimental_run_tf_function\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdistribute\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   3606\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtarget_tensors\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, or \u001b[39m\u001b[39m\"\u001b[39m\u001b[39msample_weight_mode\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   3607\u001b[0m     )\n\u001b[0;32m   3609\u001b[0m \u001b[39m# Model must be created and compiled with the same DistStrat.\u001b[39;00m\n\u001b[0;32m   3610\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt \u001b[39mand\u001b[39;00m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mhas_strategy():\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid keyword argument(s) in `compile()`: ({'model_metrics', 'model_loss', 'model_optimizer'},). Valid keyword arguments include \"cloning\", \"experimental_run_tf_function\", \"distribute\", \"target_tensors\", or \"sample_weight_mode\"."
     ]
    }
   ],
   "source": [
    "neural_net.compile(\n",
    "  model_optimizer=tf.keras.optimizers.Adam(),\n",
    "  model_loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "  model_metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit = neural_net.fit(train_dataset, epochs=6, validation_data=validation_dataset)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = neural_net.evaluate(validation_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('cnn_notebook')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e03d5c99657cd8f0367d2796ecd49a40539720e9292dc019d0e5f9d5be6ae7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
